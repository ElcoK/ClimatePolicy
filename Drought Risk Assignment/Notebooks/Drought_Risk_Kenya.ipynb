{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">  \n",
    "Notebook by Sem Vijverberg (Adapted from Ted Veldkamps and Marthe Wens) <br>\n",
    "</div>\n",
    "\n",
    "## Drought risk: ENSO impact on drought hazard for Kenya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the lecture, you were taught about ways to estimate meteorological, agricultural, and hydrological drought hazard. Either by using standardized indicators (SPI, SPEI, SSFI) or by fixed or variable threshold level methods (Q90). In this practical, you will quantify African drought using the SPI indicator and discover the impact of ENSO on Drought. \n",
    "\n",
    "Learning goals for this practicum are:\n",
    "- Learn to apply standardized methods to represent meteorological and hydrological drought hazard;\n",
    "- Learn to deal with multi-dimensional climate datasets.\n",
    "- Learn to visualize and interpret the results from drought hazard calculation methods.\n",
    "\n",
    "## Performing the exercise\n",
    "To run any script in the code-boxes below use *shift+enter*. **action** denotes where you have to make changes to particular pieces of code. When a script is running this is indicated by the * on the left side of the window.\n",
    "\n",
    "## Saving your results\n",
    "See the word answer sheet in the main repository. Make sure you safe the requested outputs (figures, tables) and answers to questions here. Furthermore, save the codes that you have created and run during this exercise (right clicking the .ipynb and select download).\n",
    "\n",
    "## Working in a Binder environment?\n",
    "Binder is super helpfull in the sense that all Python dependencies will work. **But a Binder can be unstable.** After you have made changes to the code please copy and paste that into a local text file to ensure you can easily retrieve progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any Python script (in an offline or online modus) it is necessary to import a number of Python packages that can help you with performing the calculations. \n",
    "\n",
    "**Action**: Perform the script in the code-boxes below by selecting them and press *shift+enter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import working modules\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import inspect, os, sys\n",
    "\n",
    "# Define paths\n",
    "workingfolder = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "if workingfolder.split('/')[-1] != 'Answers':\n",
    "    Notebooks_path = os.path.join(workingfolder, 'Notebooks')\n",
    "else:\n",
    "    Notebooks_path = os.path.join(workingfolder, '..', 'Notebooks')\n",
    "    \n",
    "# adding path of notebooks to sys.path\n",
    "if Notebooks_path not in sys.path:\n",
    "    sys.path.append(Notebooks_path)\n",
    "import core_pp\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['legend.fontsize'] = 'xx-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Data-folder a number of files have been prepared that serve as input for this exercise. First, open the \"precip_file\" and \"evap_file\" familiarize yourself with the data. \n",
    "\n",
    "**Action**: Make sure that path is correct. If needed, make changes to input_folder and input_file. Afterwards run the code by pressing *ctrl+enter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfolder   = os.path.join(workingfolder,'..', 'Data')\n",
    "\n",
    "# Name input file\n",
    "precip_file   = os.path.join(inputfolder,'p_1979-2018_Africa.nc')\n",
    "evap_file     = os.path.join(inputfolder,'PE_1979-2018_Africa.nc')\n",
    "country_codes = os.path.join(inputfolder,'Africa_mask_Countries.nc')\n",
    "sst_Pacific   = os.path.join(inputfolder,'sst_1979-2018_Trop_Pacific.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Loading Precipitation and Potential Evaporation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now start with loading and inspecting the input data within Python Jupyter Notebooks. You will do this in the following code-box. Press *ctrl+enter* to run the code.\n",
    "\n",
    "**Action**: Load the data into the memory of this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_m  = core_pp.import_ds_lazy(precip_file)\n",
    "evap_m    = core_pp.import_ds_lazy(evap_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Inspect the data set by plotting the climalogical rainfall and potential evaporation over Africa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: Inspecting the data\n",
    "In python you can store data in different manners. Famous data types are Lists, Numpy arrays, dictionairies, or pandas Dataframe. Each data type has advantages and disadvantage. An xarray.DataArray is super handy for multi-dimensional datasets. It is like an excell sheets, but we can add as many dimensions as we want. For normal Excell sheets, we are limited to either the row or column dimensions, xarray can handle multiple. In this case, we see the time, latitude (imagine the y-axis) and longitude (imagine the x-axis) dimensions. We can also see that units are in Meters (per day), lets convert that to mm per day get a better feeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = precip_m * 1000\n",
    "precip.name = 'Precipitation [mm/day]'\n",
    "precip.attrs['units'] = '[mm/day]'\n",
    "evap   = evap_m * 1000\n",
    "evap.name = 'Potential Evaporation [mm/day]'\n",
    "evap.attrs['units'] = '[mm/day]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we create the most simple plot of the climatological rainfall over Africa. For a climatology, we at least need 30 years. This is true particularly for precipiation, some argue we need at least 50 years to get a good climatology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(precip).mean(dim='time').plot.contourf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0:** Why do you think we need more years to get a robust climatology compared to other variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Python Package Cartopy (ccrs), we can make nice spatial maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Briefly explain what you can learn from figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "# Use Cartopy Python package to plot a map and draw country lines\n",
    "map_proj = ccrs.PlateCarree(central_longitude=10)\n",
    "\n",
    "fig = plt.figure(figsize=(30,10) )\n",
    "\n",
    "# plot precipitation\n",
    "# The integer numbers for plt.subplot refer to (n_rows, n_cols, 'index of figure') \n",
    "ax1 = plt.subplot(1, 2, 1, projection=map_proj) \n",
    "colormap = plt.cm.Blues\n",
    "precip.mean(dim='time').plot.contourf(ax=ax1, transform=ccrs.PlateCarree(), \n",
    "                             subplot_kws={'projection': map_proj},\n",
    "                             cmap=colormap)\n",
    "ax1.coastlines()\n",
    "# plot evaporation\n",
    "ax2 = plt.subplot(1, 2, 2, projection=map_proj)\n",
    "colormap = plt.cm.coolwarm_r\n",
    "evap.mean(dim='time').plot.contourf(ax=ax2, transform=ccrs.PlateCarree(), \n",
    "                             subplot_kws={'projection': map_proj},\n",
    "                             cmap=colormap)\n",
    "ax2.coastlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (precip + (evap)).mean(dim='time').plot.contourf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Retrieve data for Kenya\n",
    "First we will focus on quantifying drought in Kenya. In order to retrieve only data from that country, we load the 'country_codes'.\n",
    "\n",
    "**Action**: Load country_codes and select precipiation and evaporation of Kenya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = xr.open_dataset(country_codes).country_mask\n",
    "land_sea_mask = country >= 1\n",
    "map_proj = ccrs.PlateCarree(central_longitude=10)\n",
    "colormap = plt.cm.gist_ncar_r\n",
    "fig = plt.figure(figsize=(15,10) )\n",
    "\n",
    "ax1 = plt.subplot(1, 1, 1, projection=map_proj)\n",
    "country.plot(ax=ax1, transform=ccrs.PlateCarree(), \n",
    "                            subplot_kws={'projection': map_proj},\n",
    "                            cmap=colormap,\n",
    "                            vmin=0, vmax=country.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *country* file contains an attribute called 'country_code'. The following cell will convert this information into a Python dictionairy. A python dictionairy consists of keys and items in the following format: {*key1* : *item1*}. In the following, the key refers to an abbreviation of the country nams and the item refers to the numbers in the country mask. \n",
    "\n",
    "We will focus the coming analysis on Kenia, which has the *key* KE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = country.attrs['country_code']\n",
    "codes_dict = {}\n",
    "for i in np.arange(0,len(c_list),2):\n",
    "    codes_dict[str(c_list[i])] = int(c_list[i+1])\n",
    "codes_dict['KE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we need to gridpoints corresponding to number 115 in the *country* xarray. We will make a boolean mask (consisting of only True and False), which will only be True at Kenya. We can use that mask to select the gridcells of the *precip* and *evap* xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kenya_mask = country.values == 115\n",
    "Kenya_mask = (('latitude', 'longitude'), Kenya_mask )\n",
    "precip.coords['mask'] = Kenya_mask\n",
    "pr_kenya = precip.where(precip.mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Add the Kenya country mask to the evaporation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_proj = ccrs.PlateCarree(central_longitude=10)\n",
    "\n",
    "fig = plt.figure(figsize=(30,10) )\n",
    "\n",
    "# plot precipitation\n",
    "ax1 = plt.subplot(1, 2, 1, projection=map_proj)\n",
    "colormap = plt.cm.Blues\n",
    "pr_kenya.mean(dim='time').plot(ax=ax1, transform=ccrs.PlateCarree(), \n",
    "                             subplot_kws={'projection': map_proj},\n",
    "                             cmap=colormap,\n",
    "                             vmin=-3, vmax=3)\n",
    "ax1.coastlines()\n",
    "# plot evaporation\n",
    "ax2 = plt.subplot(1, 2, 2, projection=map_proj)\n",
    "colormap = plt.cm.RdBu\n",
    "ev_kenya.mean(dim='time').plot(ax=ax2, transform=ccrs.PlateCarree(), \n",
    "                             subplot_kws={'projection': map_proj},\n",
    "                             cmap=colormap,\n",
    "                                vmin=-3, vmax=3)\n",
    "ax2.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: SPI for a single gridcell in Kenya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to calculate SPI\n",
    "\n",
    "1. SPI is bases on temporally aggregated values of precipitation (in mm). Common temporal aggregations are 1, 3 and 6 months, but their are no stricts rules. You can simply call it SPI-x, with x representing the amount of accumulated months.\n",
    "2. We then fit a distribution (across years), often the gamma distribution is used. However, given the strong heterogeneity around the globe, this distribution is not always the best. Preferably one tests different distributions, and chooses the best one. For this exercise, we will stick to Gamma distribution.  \n",
    "3. The fitted distribution gives us the probability that a value occurs, this probability is transformed to standard normal distribution to ease interpretation. Stagge et al. (2015) pointed out that we should remove all zeros when we do this transformation to ensure that the mean of the new standard normal distribution will be zero.   \n",
    "\n",
    "Note, since we are working with daily data, we take aggregation over i-days, representing the monthly aggregations. i.e. 1, 3, 6, which are considered equivalent to 31, 91, 183 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First get a single gridcell from Kenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to do a technical conversion to retrieve only the gridcells of Kenya\n",
    "np_pr_kenya = np.reshape(pr_kenya.values, (pr_kenya.time.size, -1))\n",
    "mask_NanIsFalse   = ~np.isnan(np_pr_kenya) # where are values not NaN \n",
    "# select only where values not NaN (Kenya gridcells)\n",
    "pr_kenya_v  = np_pr_kenya[mask_NanIsFalse].reshape( pr_kenya.time.size, -1) \n",
    "df_kenya = pd.DataFrame(pr_kenya_v, index=pr_kenya.time) # make a pandas dataframe\n",
    "\n",
    "# single gridcell\n",
    "i_gridcell = 0\n",
    "df_gs = df_kenya[i_gridcell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - step 1: Apply rolling sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPI_aggr = 1\n",
    "months = [1, 3, 6]\n",
    "days = [31, 91, 183]\n",
    "SPI_to_days = {months[i]:days[i] for i in range(len(months))}\n",
    "SPI_window = SPI_to_days[SPI_aggr]\n",
    "\n",
    "# the following line calculates a sum of window size {SPI_aggr}, using equal weights. \n",
    "df_kenya = df_gs.rolling(window=SPI_window, min_periods=1, center=True, axis=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - step 2: Fit distribution for single gridcell and single day of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we are calculating SPI-{n} for each day of the year by using the rolling sum approach,\n",
    "# we only need to select a single day to represent a n-month aggregation. \n",
    "\n",
    "# to select the SPI for month m (with 1 = jan and 12 = Dec):\n",
    "m = 4\n",
    "dates = df_gs.index\n",
    "doy = int(np.mean(dates.dayofyear[dates.month == m])) # central dayofyear for the given month {m}\n",
    "\n",
    "df_gs_m = df_gs[dates.dayofyear == doy]\n",
    "\n",
    "# Fit Gamma distibution\n",
    "# note floc avoids loc being fitted, see Stagge et al. 2015\n",
    "a, loc, scale = stats.gamma.fit(df_gs_m.values, loc=0) # fit parameters of gamma distribution to SPI data \n",
    "params = [a, loc, scale]\n",
    "rv = stats.gamma(*params) # Continuous random variable class, can sample randomly from the gamma distribution we just fitted  \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,10) )\n",
    "l1=plt.plot(df_gs_m, '.k', markersize=20, color='b')\n",
    "plt.ylabel('',size=15)\n",
    "plt.title('',size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Intermediate step: Using Kolmogorov-Smirnov test to test goodness-of-fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [a, loc, scale]\n",
    "D, p = stats.kstest(df_gs_m, 'gamma', args=params)  \n",
    "print('pvalue: ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss whether the presented pvalue is sufficient. What hypothesis does the K-S test actually tests? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gs_m is precipitation of single gridcel for a single month, with a single value for each year\n",
    "\n",
    "fig = plt.figure(figsize=(30,10) )\n",
    "# plot observed values\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "vals = ax1.hist(df_gs_m, density=True, bins=20, label='Dist. Data')\n",
    "# plot fitted distribution\n",
    "x = np.linspace(stats.gamma.ppf(0.01, a, loc=loc, scale=scale),\n",
    "                stats.gamma.ppf(0.99, a, loc=loc, scale=scale), 100)\n",
    "ax1.plot(x[1:], rv.pdf(x)[1:], 'k-', lw=2, label='Gamma dist. fitted')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title(f\"K-S test pvalue: {round(p, 2)}\", fontsize=25)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.plot(df_gs_m,'.k');\n",
    "ax2.set_title('Raw data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - step 3: Transform to standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Transform to standard normal distribution (and account for zero values (cfr.Stagge et al. 2015))  \n",
    "indices_nonzero = np.nonzero(df_gs_m)[0]\n",
    "nyears_zero = len(df_gs_m) - np.count_nonzero(df_gs_m)\n",
    "ratio_zeros = nyears_zero / len(df_gs_m)\n",
    "\n",
    "p_zero_mean = (nyears_zero + 1) / (2 * (len(df_gs_m) + 1))           \n",
    "\n",
    "prob_gamma = (df_gs_m * 0 ) + p_zero_mean\n",
    "# probability of values, given the Gamma distribution\n",
    "prob_gamma[indices_nonzero] = ratio_zeros+((1-ratio_zeros)*rv.cdf(df_gs_m[indices_nonzero]))\n",
    "\n",
    "# Transform \n",
    "prob_std = stats.norm.ppf(prob_gamma)                                   \n",
    "prob_std[prob_std>3] = 3\n",
    "prob_std[prob_std<-3] = -3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10) )\n",
    "\n",
    "# plot observed values\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "vals = ax1.plot(df_gs_m, prob_gamma, '.k');\n",
    "ax1.set_ylabel('Cumulative probability')\n",
    "ax1.set_xlabel('Aggregated Precipitation [mm]')\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 2)\n",
    "vals = ax1.plot(prob_std, prob_gamma, '.k');\n",
    "ax1.set_ylabel('Cumulative probability')\n",
    "ax1.set_xlabel('SPI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Explain in words what information you can deduce from these two figures? For example, why is the Gamma distribution increasing sharply at low values compared to the standard normal distribution?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Explain in your own words why the conversion from gamma to standard normal distribution is desirable when quantifying drought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Functions to calculate SPI \n",
    "What we have done in step 3, will now be written into a funciton which calculates SPI for a given gridcell on a specific day of year. Subsequently, we have written down a fuction that loops over all gridcells and days and store the results in an new xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SPI_gs_doy(df_gs, doy):\n",
    "    '''\n",
    "    Calculates SPI for a given gridcell on a specific day of year\n",
    "    '''\n",
    "    # step 2: \n",
    "    # fit distribution\n",
    "    df_gs_m = df_gs[df_gs.index.dayofyear == doy]\n",
    "    # note floc avoids loc being fitted, see Stagge et al. 2015\n",
    "    params = stats.gamma.fit(df_gs_m.values, loc=0) # fit parameters of gamma distribution to SPI data \n",
    "    rv = stats.gamma(*params) # Continuous random variable class, can sample randomly from the gamma distribution we just fitted  \n",
    "    \n",
    "    # Account for zero values (cfr.Stagge et al. 2015))  \n",
    "    indices_nonzero = np.nonzero(df_gs_m)[0]\n",
    "    nyears_zero = len(df_gs_m) - np.count_nonzero(df_gs_m)\n",
    "    ratio_zeros = nyears_zero / len(df_gs_m)\n",
    "\n",
    "    p_zero_mean = (nyears_zero + 1) / (2 * (len(df_gs_m) + 1))           \n",
    "    prob_gamma = (df_gs_m * 0 ) + p_zero_mean\n",
    "    # probability of values, given the Gamma distribution and excluding zeros\n",
    "    prob_gamma[indices_nonzero] = ratio_zeros+((1-ratio_zeros)*rv.cdf(df_gs_m[indices_nonzero]))\n",
    "    \n",
    "    # Step 3:\n",
    "    # Transform Gamma probabilities to standard normal probabilities\n",
    "    prob_std = stats.norm.ppf(prob_gamma)                                   \n",
    "    prob_std[prob_std>3] = 3\n",
    "    prob_std[prob_std<-3] = -3 \n",
    "    return prob_std\n",
    "\n",
    "def calc_SPI_gs_pentad(df_gs, p):\n",
    "    '''\n",
    "    For a given dataframe, the centered single date for the pentad is taken and passed to calc_SPI_gs_doy(df_gs, doy).\n",
    "    There 73 pentads of 5 in 365 days, hence p should be in range (0,73)\n",
    "    '''\n",
    "    i = np.arange(0,365, 5)[p]\n",
    "    dates = df_gs.index\n",
    "    idx_pentad = np.arange(i, dates.size, 365, dtype=int)\n",
    "    doy = int(np.mean(dates[idx_pentad].dayofyear)) # central dayofyear for the given month {m}\n",
    "    prob_std = calc_SPI_gs_doy(df_gs, doy)\n",
    "    return prob_std\n",
    "\n",
    "def calc_SPI_gs_month(df_gs, month):\n",
    "    '''\n",
    "    For a given dataframe,  the centered single date for the {month} is taken and passed to calc_SPI_gs_doy(df_gs, doy).\n",
    "    There 73 pentads of 5 in 365 days, hence p should be in range (0,73)\n",
    "    '''\n",
    "    # to select the SPI for month m (with 1 = jan and 12 = Dec):\n",
    "    m = month\n",
    "    dates = df_gs.index\n",
    "    doy = int(np.mean(dates.dayofyear[dates.month == m])) # central dayofyear for the given month {m}\n",
    "    prob_std = calc_SPI_gs_doy(df_gs, doy)\n",
    "    return prob_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SPI_from_daily(xarray, SPI_aggr, freq='months'):\n",
    "    '''\n",
    "    Function extracts the values of xarray that are not nans \n",
    "    and will pass them to calc_SPI_gs_. \n",
    "    One can choose to calculate SPI for all days, or for each month or pentad (5-day bins).\n",
    "    Advisably, stick to monthly or pentads, for computational reasons.\n",
    "    '''\n",
    "\n",
    "    # Step 1: SPI monthly windowed aggregation\n",
    "    months = [1, 2, 3, 6, 9, 12, 24]\n",
    "    days = [31, 61, 91, 183, 274, 365, 730]\n",
    "    SPI_to_days = {months[i]:days[i] for i in range(len(months))}\n",
    "    SPI_window = SPI_to_days[SPI_aggr]\n",
    "    dates = pd.to_datetime(xarray.time.values)\n",
    "    n_yrs = np.unique(dates.year)\n",
    "\n",
    "    # Finding only the non-NaN gridcells \n",
    "    np_pr = np.reshape(xarray.values, (dates.size, -1))\n",
    "    n_gs_total = np_pr.shape[1]\n",
    "    mask_NanIsFalse   = ~np.isnan(np_pr) # where are values not NaN \n",
    "    # Select only the non-NaN gridcells, and cast to dataframe.\n",
    "    pr_no_nans  = np_pr[mask_NanIsFalse].reshape( dates.size, -1) \n",
    "    df_no_nans = pd.DataFrame(pr_no_nans, index=dates) # make a pandas dataframe\n",
    "    # Calculate a sum of window size {SPI_aggr[months]}, using equal weights. \n",
    "    df_no_nans = df_no_nans.rolling(window=SPI_window, min_periods=1, center=True, axis=0).sum()\n",
    "    n_gs_no_nans = int(df_no_nans.columns.size)\n",
    "\n",
    "    # loop over gridcells cells and specific day of year\n",
    "    for i, gs in enumerate(df_no_nans.columns):\n",
    "        if freq == 'daily':\n",
    "            new_dates = dates\n",
    "            if i == 0:\n",
    "                np_tofill = np.zeros( dates.dayofyear.size,  n_gs_no_nans)\n",
    "            for doy in np.unique(dates.dayofyear):\n",
    "                mask_doy = dates.dayofyear == doy\n",
    "                np_tofill[mask_doy,i] = calc_SPI_gs_doy(df_no_nans[gs], doy)\n",
    "                \n",
    "        if freq == 'pentads':\n",
    "            np_tofill = np.zeros( (int(dates.size/5),  n_gs_no_nans)) \n",
    "            new_dates = []\n",
    "            for p, d in enumerate(range(0,365,5)):\n",
    "                x = np.arange(d, dates.size, 365, dtype=int)\n",
    "                new_dates.append(dates[x])\n",
    "                if i == 0:\n",
    "                    mask_NanIsFalse = mask_NanIsFalse[:new_dates.size]\n",
    "                    idx_fill = np.arange(p, int(dates.size/5), 73)\n",
    "                np_tofill[idx_fill,i] = calc_SPI_gs_pentad(df_no_nans[gs], p)\n",
    "            new_dates = pd.to_datetime(sorted(np.concatenate(new_dates)))\n",
    "            \n",
    "        if freq == 'months':\n",
    "            new_dates = xarray.resample(time='M', \n",
    "                                        label='left').mean().time.values \n",
    "            new_dates = pd.to_datetime(new_dates + pd.Timedelta('1D'))\n",
    "            if i == 0:\n",
    "                mask_NanIsFalse = mask_NanIsFalse[:new_dates.size]\n",
    "                np_tofill = np.zeros( (new_dates.size,  n_gs_no_nans)) \n",
    "            for m in range(1,13):\n",
    "                idx_fill = np.arange(m-1, new_dates.size, 12)\n",
    "                np_tofill[idx_fill,i] = calc_SPI_gs_month(df_no_nans[gs], m)\n",
    "\n",
    "            \n",
    "\n",
    "    # return values in there original position\n",
    "    np_time_space = np.zeros( (new_dates.size, n_gs_total) )\n",
    "    np_time_space[mask_NanIsFalse] = np_tofill.ravel()\n",
    "    np_latlon = np.reshape(np_time_space, (new_dates.size, \n",
    "                                           xarray.latitude.size, \n",
    "                                           xarray.longitude.size))\n",
    "\n",
    "    # return xarray with same dimensions:\n",
    "    dates = pd.to_datetime(new_dates)\n",
    "    SPI_xr = xr.DataArray(np_latlon, coords=[dates, \n",
    "                                             xarray.latitude, xarray.longitude],\n",
    "                            dims=['time', 'latitude', 'longitude'])\n",
    "    mask_nans = mask_NanIsFalse[0].reshape(xarray.latitude.size, \n",
    "                                           xarray.longitude.size)\n",
    "    mask = (('latitude', 'longitude'), mask_nans)\n",
    "    SPI_xr['mask'] = mask\n",
    "    SPI_xr.attrs['units'] = '[std]'\n",
    "    return SPI_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Calculate SPI for Kenya gridpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPI_aggr = 6\n",
    "freq = 'months' # can be ['months', 'pentads', 'daily']\n",
    "SPI_xr = calc_SPI_from_daily(pr_kenya, SPI_aggr=SPI_aggr, freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Without any calculation, what will now be the climatological value of SPI. You can check calculating and plotting the climatology like was done above in Section 2a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Calculate ENSO 3.4 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sea Surface Temperature (sst) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_Pac  = core_pp.import_ds_lazy(sst_Pacific, format_lon='only_east')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a: Calculating a spatial mean\n",
    "When calculating a spatial mean, one has to take into account that the earth is round, \n",
    "meaning that the equally spaced gridboxes do not represent equal surfaces of the earth. Admittendly, it will not matter much for countries close to the equator, but it is the neat thing to do. The function to take into account the differences in gridbox sizes is defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_weighted(xarray):\n",
    "   # Area weighted, taking cos of latitude in radians     \n",
    "    coslat = np.cos(np.deg2rad(xarray.coords['latitude'].values)).clip(0., 1.)\n",
    "    area_weights = np.tile(coslat[..., np.newaxis],(1,xarray.longitude.size))\n",
    "    area_weights = area_weights / area_weights.mean()\n",
    "    return xr.DataArray(xarray.values * area_weights, coords=xarray.coords, \n",
    "                          dims=xarray.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like done before, we can easily select the dimensions over which we want to do a computation. Thus for calculating the spatial mean we simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_enso = sst_Pac.sel(latitude=slice(10,-10), longitude=slice(190,240))\n",
    "# non-normalized timeseries\n",
    "Enso_34_raw = area_weighted(sst_enso).mean(dim=('latitude','longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b: Calculate SPI timeseries for Kenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPI_Kenya_ts = area_weighted(SPI_xr).where(SPI_xr['mask']).mean(\n",
    "                                            dim=('latitude','longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: In the cell below, standardize both the SPI and ENSO_34_raw timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we select the dates for which the SPI is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enso_34   = Enso_34_all.sel(time=SPI_Kenya_ts.time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Link between ENSO and Kenya drought?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a: Visualizing timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "\n",
    "fig = plt.figure(figsize=(30,10) )\n",
    "dates = pd.to_datetime(Enso_34.time.values)\n",
    "l1=plt.plot(dates, Enso_34, color='b', label='ENSO',\n",
    "           linewidth=3)\n",
    "l2=plt.plot(dates, SPI_Kenya_ts, color='r', label='SPI_Kenya', alpha=0.6)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('',size=15)\n",
    "plt.title('',size=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Copy-past the code to make figure 2 and add a smoothend timeseries of SPI by applying a rolling mean in the figure below. You should be able to make this work using examples from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b: Cross-correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we calculate a cross-correlation matrix using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([Enso_34.values[:,None], \n",
    "                       SPI_Kenya_ts.values[:,None], \n",
    "                       SPI_Kenya_ts_rm6.values[:,None]], axis=1)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Enso_34', 'SPI_Kenya', \n",
    "                                 f\"SPI_Kenya rm{rolling_mean_smooth}\"],\n",
    "            index=SPI_Kenya_ts['time'])\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** 6: What is your interpretation of the results of 7a and 7b? (insert figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stats.pearsonr(ts1,ts2) you can calculate the Pearson correlation coefficient and the concomitant p-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Find out if ENSO is indeed significantly influencing drought in Kenya? (insert screenshot of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**: What do you think that would happen if we aggregate SPI over very long or short time windows? Re-run the code starting from Step 5 and fill in 1 months and 24 months. What happends to the correlation coefficient (r) and the p-value? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
